{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install h5py\n!pip install typing-extensions\n!pip install wheel","metadata":{"execution":{"iopub.status.busy":"2022-03-12T06:37:59.087249Z","iopub.execute_input":"2022-03-12T06:37:59.087643Z","iopub.status.idle":"2022-03-12T06:38:17.378062Z","shell.execute_reply.started":"2022-03-12T06:37:59.087608Z","shell.execute_reply":"2022-03-12T06:38:17.377075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install fastai","metadata":{"execution":{"iopub.status.busy":"2022-03-12T06:46:34.159251Z","iopub.execute_input":"2022-03-12T06:46:34.159625Z","iopub.status.idle":"2022-03-12T06:47:38.509388Z","shell.execute_reply.started":"2022-03-12T06:46:34.159592Z","shell.execute_reply":"2022-03-12T06:47:38.508495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade torch==1.8.0           ","metadata":{"execution":{"iopub.status.busy":"2022-03-12T06:52:13.645101Z","iopub.execute_input":"2022-03-12T06:52:13.645558Z","iopub.status.idle":"2022-03-12T06:52:47.382393Z","shell.execute_reply.started":"2022-03-12T06:52:13.645512Z","shell.execute_reply":"2022-03-12T06:52:47.381456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchvision\nprint(torch.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T06:44:32.634297Z","iopub.execute_input":"2022-03-12T06:44:32.634664Z","iopub.status.idle":"2022-03-12T06:44:32.640106Z","shell.execute_reply.started":"2022-03-12T06:44:32.63463Z","shell.execute_reply":"2022-03-12T06:44:32.639222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nprint(sys.version)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T06:27:59.501307Z","iopub.execute_input":"2022-03-12T06:27:59.501715Z","iopub.status.idle":"2022-03-12T06:27:59.506198Z","shell.execute_reply.started":"2022-03-12T06:27:59.501682Z","shell.execute_reply":"2022-03-12T06:27:59.50513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\nimport torch.nn as nn\nimport torchvision\nimport torch.nn.functional as F\nimport numpy as np \nfrom matplotlib import pyplot as plt\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport torch.optim as optim\nfrom torch.utils.data import random_split\nfrom torchvision import models,datasets\nimport os\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom torch.optim import lr_scheduler\nfrom torchvision.utils import make_grid","metadata":{"execution":{"iopub.status.busy":"2022-03-12T06:24:54.72697Z","iopub.execute_input":"2022-03-12T06:24:54.727375Z","iopub.status.idle":"2022-03-12T06:24:56.443238Z","shell.execute_reply.started":"2022-03-12T06:24:54.727339Z","shell.execute_reply":"2022-03-12T06:24:56.442396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"USE_GPU = True\n\nif USE_GPU and torch.cuda.is_available():\n    print('using device: cuda')\nelse:\n    print('using device: cpu')\n\ndevice = torch.device(\"cuda:0\" if USE_GPU else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-03-12T06:24:58.776177Z","iopub.execute_input":"2022-03-12T06:24:58.776626Z","iopub.status.idle":"2022-03-12T06:24:58.781984Z","shell.execute_reply.started":"2022-03-12T06:24:58.77658Z","shell.execute_reply":"2022-03-12T06:24:58.781123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Hyperparamters\nBATCH_SIZE = 4","metadata":{"execution":{"iopub.status.busy":"2022-03-12T06:24:45.894534Z","iopub.execute_input":"2022-03-12T06:24:45.895003Z","iopub.status.idle":"2022-03-12T06:24:45.898753Z","shell.execute_reply.started":"2022-03-12T06:24:45.894967Z","shell.execute_reply":"2022-03-12T06:24:45.897848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision.transforms as transforms\ntrain_transform = transforms.Compose([\n        transforms.Resize((256,256)),\n        #transforms.RandomResizedCrop(256),\n        #transforms.RandomHorizontalFlip(),\n        #transforms.ColorJitter(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])\n])\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-12T06:25:03.309583Z","iopub.execute_input":"2022-03-12T06:25:03.309899Z","iopub.status.idle":"2022-03-12T06:25:03.314848Z","shell.execute_reply.started":"2022-03-12T06:25:03.309871Z","shell.execute_reply":"2022-03-12T06:25:03.313729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Flare(Dataset):\n    def __init__(self, flare_dir, wf_dir,transform = None):\n        self.flare_dir = flare_dir\n        self.wf_dir = wf_dir\n        self.transform = transform\n        self.flare_img = os.listdir(flare_dir)\n        self.wf_img = os.listdir(wf_dir)\n        \n    def __len__(self):\n         return len(self.flare_img)\n    def __getitem__(self, idx):\n        f_img = Image.open(os.path.join(self.flare_dir, self.flare_img[idx])).convert(\"RGB\")\n        for i in self.wf_img:\n            if (self.flare_img[idx].split('.')[0][4:] == i.split('.')[0]):\n                wf_img = Image.open(os.path.join(self.wf_dir, i)).convert(\"RGB\")\n                break\n        f_img = self.transform(f_img)\n        wf_img = self.transform(wf_img)\n        \n        return f_img, wf_img          ","metadata":{"execution":{"iopub.status.busy":"2022-03-12T06:25:05.098722Z","iopub.execute_input":"2022-03-12T06:25:05.099043Z","iopub.status.idle":"2022-03-12T06:25:05.109294Z","shell.execute_reply.started":"2022-03-12T06:25:05.099013Z","shell.execute_reply":"2022-03-12T06:25:05.108389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"flare_dir = '../input/flaredataset/Flare/Flare_img'\nwf_dir = '../input/flaredataset/Flare/Without_Flare_'\nflare_img = os.listdir(flare_dir)\nwf_img = os.listdir(wf_dir)\nwf_img.sort()\nflare_img.sort()\nprint(wf_img[0])\n\ntrain_ds = Flare(flare_dir, wf_dir,train_transform)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_ds,\n                                           batch_size=BATCH_SIZE, \n                                           shuffle=True)\n\nprint(train_ds)\nprint(train_loader)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T06:25:07.587718Z","iopub.execute_input":"2022-03-12T06:25:07.588041Z","iopub.status.idle":"2022-03-12T06:25:08.109827Z","shell.execute_reply.started":"2022-03-12T06:25:07.588011Z","shell.execute_reply":"2022-03-12T06:25:08.108955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i,l = next(iter(train_loader))\nprint(i.min())\nprint(len(train_loader))\nprint(i.shape)\nprint(l.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T06:25:10.819057Z","iopub.execute_input":"2022-03-12T06:25:10.819489Z","iopub.status.idle":"2022-03-12T06:25:11.081545Z","shell.execute_reply.started":"2022-03-12T06:25:10.819448Z","shell.execute_reply":"2022-03-12T06:25:11.080094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy\nsamples, labels = iter(train_loader).next()\nplt.figure(figsize=(16,24))\ngrid_imgs = torchvision.utils.make_grid(samples,nrow = 4, normalize = True)\nnp_grid_imgs = grid_imgs.numpy()\n# in tensor, image is (batch, width, height), so you have to transpose it to (width, height, batch) in numpy to show it.\nplt.imshow(numpy.transpose(np_grid_imgs, (1,2,0)))\nplt.axis('off')\nplt.figure(figsize=(16,24))\ngrid_imgs = torchvision.utils.make_grid(labels,nrow=4, normalize = True)\nnp_grid_imgs = grid_imgs.numpy()\n# in tensor, image is (batch, width, height), so you have to transpose it to (width, height, batch) in numpy to show it.\nplt.imshow(numpy.transpose(np_grid_imgs, (1,2,0)))\nplt.axis('off')","metadata":{"execution":{"iopub.status.busy":"2022-03-12T06:25:13.234842Z","iopub.execute_input":"2022-03-12T06:25:13.235169Z","iopub.status.idle":"2022-03-12T06:25:13.908154Z","shell.execute_reply.started":"2022-03-12T06:25:13.235137Z","shell.execute_reply":"2022-03-12T06:25:13.907218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Model\n#We are going to make a couple ofsmall changes in the implementation, compared to the original unet paper, we are going to use padded convolution and not downsize and we'll use BatchNorm.\nclass DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DoubleConv, self).__init__()\n        self.conv = nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias = False), #Same padding, bias is False because we are using BatchNorm\n        nn.BatchNorm2d(out_channels),\n        nn.LeakyReLU(0.2, inplace = True),\n        nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias = False),\n        nn.BatchNorm2d(out_channels),\n        nn.LeakyReLU(0.2, inplace = True)\n        )\n        \n    def forward(self, x):\n        return self.conv(x)\n    \nclass UNET(nn.Module):\n    def __init__(self, in_channels=3, out_channels=3, features = [32,64,128,256]):\n        super(UNET, self).__init__()\n        #Define two lists to store all the Conv layers and also define a pooling layer\n        self.ups = nn.ModuleList()\n        self.downs = nn.ModuleList() \n        self.pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n        \n        #Define downsampling\n        for feature in features:\n            self.downs.append(DoubleConv(in_channels,feature)),\n            in_channels = feature\n                \n        #Define Upsampling\n        for feature in reversed(features):\n            #Set kernel_size and stride to double the image_height and image_width\n            self.ups.append(\n            nn.ConvTranspose2d(feature*2,feature,kernel_size = 2,stride = 2)\n            )\n            self.ups.append(DoubleConv(feature*2,feature))\n        #This is the layer which is at the bottom of the U shape\n        self.bottleneck = DoubleConv(features[-1],features[-1]*2)\n        #Set kernel_size to maintain the height and width of the image\n        self.final_conv = nn.Conv2d(features[0],out_channels,kernel_size = 1)\n        \n    def forward(self,x):\n        skip_connections = []\n        for down in self.downs:\n            x = down(x)\n            skip_connections.append(x) #To remember the Conv we need in the skip connections\n            x = self.pool(x)\n            \n        x = self.bottleneck(x)\n        skip_connections = skip_connections[::-1]\n            \n        for idx in range(0,len(self.ups),2):\n            x = self.ups[idx](x)\n            skip_connection = skip_connections[idx//2]\n            #But what if input image dimensions are not divisible by 2^^4?\n            if x.shape != skip_connection.shape:\n                x = F.resize(x, size=skip_connection.shape[2:])#The [2:] gets rid of the batch size and number of dim\n            \n            concat_skip = torch.cat((skip_connection, x),dim = 1)\n            x = self.ups[idx+1](concat_skip)\n            \n        return self.final_conv(x)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T06:25:19.409213Z","iopub.execute_input":"2022-03-12T06:25:19.409553Z","iopub.status.idle":"2022-03-12T06:25:19.423132Z","shell.execute_reply.started":"2022-03-12T06:25:19.409516Z","shell.execute_reply":"2022-03-12T06:25:19.422121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Discriminator\nclass FeatureMapBlock(nn.Module):\n\n    def __init__(self, input_channels, output_channels):\n        super(FeatureMapBlock, self).__init__()\n        self.conv = nn.Conv2d(input_channels, output_channels, kernel_size=1)\n\n    def forward(self, x):\n  \n        x = self.conv(x)\n        return x\n\nclass ContractingBlock(nn.Module):\n\n    def __init__(self, input_channels, use_dropout=False, use_bn=True):\n        super(ContractingBlock, self).__init__()\n        self.conv1 = nn.Conv2d(input_channels, input_channels * 2, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(input_channels * 2, input_channels * 2, kernel_size=3, padding=1)\n        self.activation = nn.LeakyReLU(0.2)\n        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n        if use_bn:\n            self.batchnorm = nn.BatchNorm2d(input_channels * 2)\n        self.use_bn = use_bn\n        if use_dropout:\n            self.dropout = nn.Dropout()\n        self.use_dropout = use_dropout\n\n    def forward(self, x):\n        x = self.conv1(x)\n        if self.use_bn:\n            x = self.batchnorm(x)\n        if self.use_dropout:\n            x = self.dropout(x)\n        x = self.activation(x)\n        x = self.conv2(x)\n        if self.use_bn:\n            x = self.batchnorm(x)\n        if self.use_dropout:\n            x = self.dropout(x)\n        x = self.activation(x)\n        x = self.maxpool(x)\n        return x\n\nclass Discriminator(nn.Module):\n\n    def __init__(self, input_channels, hidden_channels=8):\n        super(Discriminator, self).__init__()\n        self.upfeature = FeatureMapBlock(input_channels, hidden_channels)\n        self.contract1 = ContractingBlock(hidden_channels, use_bn=False)\n        self.contract2 = ContractingBlock(hidden_channels * 2)\n        self.contract3 = ContractingBlock(hidden_channels * 4)\n        self.contract4 = ContractingBlock(hidden_channels * 8)\n        #### START CODE HERE ####\n        self.final = nn.Conv2d(hidden_channels * 16, 1, kernel_size=1)\n        #### END CODE HERE ####\n\n    def forward(self, x, y):\n        x = torch.cat([x, y], axis=1)\n        x0 = self.upfeature(x)\n        x1 = self.contract1(x0)\n        x2 = self.contract2(x1)\n        x3 = self.contract3(x2)\n        x4 = self.contract4(x3)\n        xn = self.final(x4)\n        return xn","metadata":{"execution":{"iopub.status.busy":"2022-03-12T06:25:26.706096Z","iopub.execute_input":"2022-03-12T06:25:26.706545Z","iopub.status.idle":"2022-03-12T06:25:26.729969Z","shell.execute_reply.started":"2022-03-12T06:25:26.706474Z","shell.execute_reply":"2022-03-12T06:25:26.728967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fe = models.resnet18(pretrained=True)\nfor param in fe.parameters():\n\tparam.requires_grad = True\nnum_ftrs = fe.fc.in_features\nclass ResNet18(nn.Module):\n  def __init__(self):\n    super(ResNet18, self).__init__()\n    self.features = torch.nn.Sequential(*list(fe.children())[:-2])\n    self.conv1 = nn.Conv2d(512,3, 3,1,1)\n    self.pool =  nn.AdaptiveAvgPool2d(output_size=(1,1))\n    self.drop1 = nn.Dropout(0.3)\n    self.fc1 = nn.Linear(3,128)\n    self.drop2 = nn.Dropout(0.2)\n    self.fc2 = nn.Linear(128,1)\n    #We did not add a softmax layer here because the CrossEntropy Loss function contains a softmax, so if you want \n    #to test output, you will have to add a softmax block in addition to the model block\n    \n  def forward(self,x):\n    x = self.features(x)\n    x = self.conv1(x)\n    x = self.pool(x)\n    x = x.view(x.shape[0],3)\n    \n    x = self.drop1(x)\n    x = F.relu(self.fc1(x))\n    x = self.drop2(x)\n    x = F.sigmoid(self.fc2(x))    \n    return x\n\n\ncriterion_cl = nn.MSELoss()    \n\nresnet_model = ResNet18()\nresnet_model = resnet_model.to(device) \n\nc1 = resnet_model.features\nc2 = resnet_model.conv1","metadata":{"execution":{"iopub.status.busy":"2022-03-12T06:25:29.899545Z","iopub.execute_input":"2022-03-12T06:25:29.899877Z","iopub.status.idle":"2022-03-12T06:25:29.975828Z","shell.execute_reply.started":"2022-03-12T06:25:29.899848Z","shell.execute_reply":"2022-03-12T06:25:29.974574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torchsummary\nfrom torchsummary import summary\nsummary(efficientnet_b0,(3,256,256))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    def get_gen_loss(gen, disc, real, condition, adv_criterion, recon_criterion, lambda_recon,c1,c2):\n        fake = gen(condition)\n        d_fake = disc(fake, condition)\n        adv_loss = adv_criterion(d_fake, torch.ones_like(d_fake))\n\n        x_fl = c1(fake)\n       # fm_fl = c2(x_fl)\n        x_og = c1(real)\n       # fm_og = c2(x_og)\n        recon_loss = 0.8*recon_criterion(x_og, x_fl)+0.2*recon_criterion(fake,real)\n        gen_loss = adv_loss + lambda_recon*recon_loss\n        return gen_loss\n\n\n    def show_tensor_images(image_tensor, num_images=5, size=(1, 28, 28)):\n        '''\n        Function for visualizing images: Given a tensor of images, number of images, and\n        size per image, plots and prints the images in an uniform grid.\n        '''\n        image_shifted = image_tensor\n        image_unflat = image_shifted.detach().cpu().view(-1, *size)\n        image_grid = make_grid(image_unflat[:num_images], nrow=5, normalize = True)\n        plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n        plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\n# New parameters\nadv_criterion = nn.BCEWithLogitsLoss() \nrecon_criterion = nn.L1Loss() \nlambda_recon = 50\n\nn_epochs = 20\ninput_dim = 3\nreal_dim = 3\ndisplay_step = 2000\nbatch_size = 4\nlrg =1e-5\nlrd = 5e-8\nimage_dim = 256\nd_scaler = torch.cuda.amp.GradScaler()\ng_scaler = torch.cuda.amp.GradScaler()\ngen = UNET(input_dim, real_dim).to(device)\ngen_opt = torch.optim.Adam(gen.parameters(), lr=lrg)\ndisc = Discriminator(input_dim + real_dim).to(device)\ndisc_opt = torch.optim.Adam(disc.parameters(), lr=lrd)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#gen.load_state_dict(torch.load('../input/model-1/FM_UNET_P2P_40_epochs_256px.pt'))\n\n#disc.load_state_dict(torch.load('../input/model-2/FM_Disc_P2P_40_epochs_256px.pt'))\n\n#resnet_model.load_state_dict(torch.load('../input/flare-resnet-classifier/Flare_Classifier_25_epochs.pt'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy\nsamples, labels = iter(train_loader).next()\nsamples = samples.to(device)\nout = efficientnet_model(samples)\ni = c1(samples)\nj = c2(i).detach()\n\nplt.figure(figsize=(16,24))\ngrid_imgs = torchvision.utils.make_grid(samples.cpu(), normalize = True)\n# in tensor, image is (batch, width, height), so you have to transpose it to (width, height, batch) in numpy to show it.\nplt.imshow(numpy.transpose(grid_imgs, (1,2,0)))\n\nplt.figure(figsize=(16,24))\ngrid_imgs = torchvision.utils.make_grid(j.cpu(), normalize = True)\n# in tensor, image is (batch, width, height), so you have to transpose it to (width, height, batch) in numpy to show it.\nplt.imshow(numpy.transpose(grid_imgs, (1,2,0)))\n\nprint(out)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from skimage import color\nimport numpy as np\n\ndef train(save_model=False):\n    mean_generator_loss = 0\n    mean_discriminator_loss = 0\n    cur_step = 0\n\n    for epoch in range(n_epochs):\n        # Dataloader returns the batches\n        for condition, real in tqdm(train_loader):\n            cur_batch_size = len(condition)\n            condition = condition.to(device)\n            real = real.to(device)\n            gen.train()\n            disc.train()\n            #decaying_noise = torch.randn(real_.size(0),1,image_dim,image_dim).to(device)*(0.9**(50*(epoch+1)))\n            #decaying_noise_ = torch.randn(condition.size(0),1,image_dim,image_dim).to(device)*(0.9**(50*(epoch+1))) \n            #real = real_+decaying_noise\n            #condition_ = condition+decaying_noise_\n            ### Update discriminator ###\n            disc_opt.zero_grad() # Zero out the gradient before backpropagation\n            with torch.no_grad():\n                fake = gen(condition)\n            disc_fake_hat = disc(fake.detach(), condition) # Detach generator\n            disc_fake_loss = adv_criterion(disc_fake_hat, torch.zeros_like(disc_fake_hat))\n            disc_real_hat = disc(real, condition)\n            disc_real_loss = adv_criterion(disc_real_hat, torch.ones_like(disc_real_hat))\n            disc_loss = (disc_fake_loss + disc_real_loss) / 2\n            disc_loss.backward(retain_graph=True) # Update gradients\n            disc_opt.step() # Update optimizer\n\n            ### Update generator ###\n            gen_opt.zero_grad()\n            gen_loss = get_gen_loss(gen, disc, real, condition, adv_criterion, recon_criterion, lambda_recon,c1,c2)\n            gen_loss.backward() # Update gradients\n            gen_opt.step() # Update optimizer\n\n            # Keep track of the average discriminator loss\n            mean_discriminator_loss += disc_loss.item() / display_step\n            # Keep track of the average generator loss\n            mean_generator_loss += gen_loss.item() / display_step\n            \n            ### Visualization code ###\n            if cur_step % display_step == 0:\n                if cur_step > 0:\n                    print(f\"Epoch {epoch+1}: Step {cur_step}: Generator (U-Net) loss: {mean_generator_loss}, Discriminator loss: {mean_discriminator_loss}\")\n                else:\n                    print(\"Pretrained initial state\")\n                gen.eval()\n                disc.eval()\n                show_tensor_images(condition, size = condition.shape[1:])\n                show_tensor_images(real, size=real.shape[1:])\n                show_tensor_images(fake, size=fake.shape[1:])\n                mean_generator_loss = 0\n                mean_discriminator_loss = 0\n    \n            cur_step += 1\ntrain()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader_ = torch.utils.data.DataLoader(dataset=train_ds,\n                                           batch_size=1, \n                                           shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l,k = next(iter(train_loader_))\nl = l.to(device).float()\nk = k.to(device).float()\ni = gen(l).cpu().detach()\n\nplt.figure(figsize=(5,5))\nplt.title(\"Ground Truth\")\nimg_grid = torchvision.utils.make_grid(k.cpu(),normalize = True)\nplt.imshow(np.transpose(img_grid,(1,2,0)))\nplt.show()\n\nplt.figure(figsize=(5,5))\nplt.title(\"Images With Flare\")\nimg_grid = torchvision.utils.make_grid(l.cpu(),normalize = True)\nplt.imshow(np.transpose(img_grid,(1,2,0)))\nplt.show()\n\nplt.figure(figsize=(5,5))\nplt.title(\"Generated_Images\")\nimg_grid = torchvision.utils.make_grid(i.cpu(),normalize = True)\nplt.imshow(np.transpose(img_grid,(1,2,0)))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l,k = next(iter(train_loader_))\nl = l.to(device).float()\nk = k.to(device).float()\ni = gen(l).cpu().detach()\n\nplt.figure(figsize=(5,5))\nplt.title(\"Ground Truth\")\nimg_grid = torchvision.utils.make_grid(k.cpu(),normalize = True)\nplt.imshow(np.transpose(img_grid,(1,2,0)))\nplt.show()\n\nplt.figure(figsize=(5,5))\nplt.title(\"Images With Flare\")\nimg_grid = torchvision.utils.make_grid(l.cpu(),normalize = True)\nplt.imshow(np.transpose(img_grid,(1,2,0)))\nplt.show()\n\nplt.figure(figsize=(5,5))\nplt.title(\"Generated_Images\")\nimg_grid = torchvision.utils.make_grid(i.cpu(),normalize = True)\nplt.imshow(np.transpose(img_grid,(1,2,0)))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l,k = next(iter(train_loader_))\nl = l.to(device).float()\nk = k.to(device).float()\ni = gen(l).cpu().detach()\n\nplt.figure(figsize=(5,5))\nplt.title(\"Ground Truth\")\nimg_grid = torchvision.utils.make_grid(k.cpu(),normalize = True)\nplt.imshow(np.transpose(img_grid,(1,2,0)))\nplt.show()\n\nplt.figure(figsize=(5,5))\nplt.title(\"Images With Flare\")\nimg_grid = torchvision.utils.make_grid(l.cpu(),normalize = True)\nplt.imshow(np.transpose(img_grid,(1,2,0)))\nplt.show()\n\nplt.figure(figsize=(5,5))\nplt.title(\"Generated_Images\")\nimg_grid = torchvision.utils.make_grid(i.cpu(),normalize = True)\nplt.imshow(np.transpose(img_grid,(1,2,0)))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_save_name = 'Disc_P2P_70_epochs_FM_256px.pt'\npath = F\".//{model_save_name}\" \ntorch.save(disc.state_dict(), path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_save_name = 'UNET_P2P_70_epochs_FM_256px.pt'\npath = F\".//{model_save_name}\" \ntorch.save(gen.state_dict(), path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FlareTest(Dataset):\n    def __init__(self, flare_dir, transform = None):\n        self.flare_dir = flare_dir\n\n        self.transform = transform\n        self.flare_img = os.listdir(flare_dir)\n      \n        \n    def __len__(self):\n        return len(self.flare_img)\n    def __getitem__(self, idx):\n        self.flare_img.sort()\n        f_img = Image.open(os.path.join(self.flare_dir, self.flare_img[idx])).convert(\"RGB\")\n      \n        f_img = self.transform(f_img)\n\n        \n        return f_img","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_transform = transforms.Compose([\n        transforms.Resize((256,256)),\n        #transforms.RandomResizedCrop(256),\n        #transforms.RandomHorizontalFlip(),\n        #transforms.ColorJitter(),\n        transforms.ToTensor(),\n        \n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dir = '../input/flaredetection/Flare/Flare_img'\n\ndir = '../input/flaredataset/Flare/Flare_img'\n#wf_dir = '../input/flaredataset/Flare/Without_Flare_'\n#'../input/flaredataset/Flare/Flare_img'\nds = FlareTest(dir, test_transform)\nloader = torch.utils.data.DataLoader(dataset=ds,\n                                           batch_size=32, \n                                           shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images = next(iter(loader))\nimages =images.to(device)\nout = gen(images).cpu().detach()\n\nplt.figure(figsize=(20,20))\nplt.title(\"Ground Truth\")\nimg_grid = torchvision.utils.make_grid(images.cpu(),normalize = True)\nplt.imshow(np.transpose(img_grid,(1,2,0)))\nplt.show()\n\nplt.figure(figsize=(20,20))\nplt.title(\"Images With Flare\")\nimg_grid = torchvision.utils.make_grid(out,normalize = True)\nplt.imshow(np.transpose(img_grid,(1,2,0)))\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testf_dir = '../input/testvvvvv/Test_Flare/Test_Flare'\nwf_dir = '../input/testvvvvv/Test_/Test_'\ntest_ds = Flare(testf_dir,wf_dir,test_transform)\ntest_loader = torch.utils.data.DataLoader(dataset=test_ds,\n                                           batch_size=32, \n                                           shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def PSNR(original, compressed):\n    mse = np.mean((original - compressed) ** 2)\n    if(mse == 0):  # MSE is zero means no noise is present in the signal .\n                  # Therefore PSNR have no importance.\n        return 100\n    max_pixel = 255.0\n    psnr = 20 * log10(max_pixel / sqrt(mse))\n    return psnr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l,k = next(iter(test_loader))\nl = l.to(device).float()\nk = k.to(device).float()\ni = np.array(gen(l).cpu().detach())\n\nplt.figure(figsize=(20,20))\nplt.title(\"Images With Flare\")\nimg_grid = torchvision.utils.make_grid(l.cpu(),normalize = True)\nplt.imshow(np.transpose(img_grid,(1,2,0)))\nplt.show()\n\nplt.figure(figsize=(20,20))\nplt.title(\"Generated_Images\")\nimg_grid = torchvision.utils.make_grid(torch.tensor(i),normalize = True)\nplt.imshow(np.transpose(img_grid,(1,2,0)))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from math import log10, sqrt\nimport cv2\nimport numpy as np\n\nl,k = next(iter(test_loader))\nl = l.to(device).float()\nk = k.to(device).float()\ni = np.array(gen(l).cpu().detach())\nsum = 0\nfor h in range(32):\n    sum += PSNR(np.array(k[h].cpu()), (i[h]))\n\n    \n    \nplt.figure(figsize=(20,20))\nplt.title(\"Ground Truth\")\nimg_grid = torchvision.utils.make_grid(k.cpu(),normalize =True)\nplt.imshow(np.transpose(img_grid,(1,2,0)))\nplt.show()\n\nplt.figure(figsize=(20,20))\nplt.title(\"Images With Flare\")\nimg_grid = torchvision.utils.make_grid(l.cpu(),normalize = True)\nplt.imshow(np.transpose(img_grid,(1,2,0)))\nplt.show()\n\nplt.figure(figsize=(20,20))\nplt.title(\"Generated_Images\")\nimg_grid = torchvision.utils.make_grid(torch.tensor(i),normalize = True)\nplt.imshow(np.transpose(img_grid,(1,2,0)))\nplt.show()\nprint(\"PSNR: \", sum/32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l,k = next(iter(test_loader))\nl = l.to(device)\nk = k.to(device)\ni = gen(l)\n\nsum = 0\nx = 0\nfor (m,n) in (test_loader):\n    x = x+1\n    m = m.to(device)\n    n = n.to(device)\n    f = np.array(gen(m).detach().cpu())\n    for j in range(m.shape[0]):\n        sum += PSNR(np.array(n[j].cpu()),f[j])\n    if(x%50 == 0):\n        print(sum/(32*x))\nprint(\"PSNR: \",sum/len(test_ds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l,k = next(iter(test_loader))\nl = l.to(device)\nk = k.to(device)\ni = gen(l)\nimport torch\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\nfrom math import exp\n\ndef gaussian(window_size, sigma):\n    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n    return gauss/gauss.sum()\n\ndef create_window(window_size, channel):\n    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n    return window\n\ndef _ssim(img1, img2, window, window_size, channel, size_average = True):\n    mu1 = F.conv2d(img1, window, padding = window_size//2, groups = channel)\n    mu2 = F.conv2d(img2, window, padding = window_size//2, groups = channel)\n\n    mu1_sq = mu1.pow(2)\n    mu2_sq = mu2.pow(2)\n    mu1_mu2 = mu1*mu2\n\n    sigma1_sq = F.conv2d(img1*img1, window, padding = window_size//2, groups = channel) - mu1_sq\n    sigma2_sq = F.conv2d(img2*img2, window, padding = window_size//2, groups = channel) - mu2_sq\n    sigma12 = F.conv2d(img1*img2, window, padding = window_size//2, groups = channel) - mu1_mu2\n\n    C1 = 0.01**2\n    C2 = 0.03**2\n\n    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\n\n    if size_average:\n        return ssim_map.mean()\n    else:\n        return ssim_map.mean(1).mean(1).mean(1)\n\nclass SSIM(torch.nn.Module):\n    def __init__(self, window_size = 11, size_average = True):\n        super(SSIM, self).__init__()\n        self.window_size = window_size\n        self.size_average = size_average\n        self.channel = 1\n        self.window = create_window(window_size, self.channel)\n\n    def forward(self, img1, img2):\n        (_, channel, _, _) = img1.size()\n\n        if channel == self.channel and self.window.data.type() == img1.data.type():\n            window = self.window\n        else:\n            window = create_window(self.window_size, channel)\n            \n            if img1.is_cuda:\n                window = window.cuda(img1.get_device())\n            window = window.type_as(img1)\n            \n            self.window = window\n            self.channel = channel\n\n\n        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)\n\ndef ssim(img1, img2, window_size = 11, size_average = True):\n    (_, channel, _, _) = img1.size()\n    window = create_window(window_size, channel)\n    \n    if img1.is_cuda:\n        window = window.cuda(img1.get_device())\n    window = window.type_as(img1)\n    \n    return _ssim(img1, img2, window, window_size, channel, size_average)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(l.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install piq","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loader = torch.utils.data.DataLoader(dataset=test_ds,\n                                           batch_size=4, \n                                           shuffle=True)\nl,k = next(iter(test_loader))\nl = l.to(device).float()\nk = k.to(device).float()\ni = gen(l)\nprint(l.shape)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom piq import ssim, SSIMLoss\nimport piq\n\nssim_index: torch.Tensor = ssim(l, k, data_range=1.)\n\nloss = SSIMLoss(data_range=1.)\noutput: torch.Tensor = loss(l, k)\noutput.backward()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"SSIM index: {ssim_index.item():0.4f}, loss: {output.item():0.4f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}